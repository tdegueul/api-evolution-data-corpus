
%% bare_conf.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[conference]{IEEEtran}
% Some Computer Society conferences also require the compsoc mode option,
% but others use the standard conference format.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
\usepackage{url}
\usepackage{todonotes}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{API Evolution Data Corpus and Tools Challenge}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{
	\IEEEauthorblockN{Kamil Jezek}
	\IEEEauthorblockA{
Department of Computer Science and Engineering \\
NTIS -- New Technologies for the Information Society \\
Faculty of Applied Sciences,
University of West Bohemia \\
Pilsen, Czech Republic \\
	kjezek@kiv.zcu.cz}

\and
	\IEEEauthorblockN{Jens Dietrich}
	\IEEEauthorblockA{School of Engineering and Advanced Technology\\
	Massey University\\
	Palmerston North, New Zealand\\
	J.B.Dietrich@massey.ac.nz} \\

}

% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
% 
%\author{\IEEEauthorblockN{Michael Shell\IEEEauthorrefmark{1},
%Homer Simpson\IEEEauthorrefmark{2},
%James Kirk\IEEEauthorrefmark{3}, 
%Montgomery Scott\IEEEauthorrefmark{3} and
%Eldon Tyrell\IEEEauthorrefmark{4}}
%\IEEEauthorblockA{\IEEEauthorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: see http://www.michaelshell.org/contact.html}
%\IEEEauthorblockA{\IEEEauthorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\IEEEauthorblockA{\IEEEauthorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\IEEEauthorblockA{\IEEEauthorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}




% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract
\begin{abstract}
Development of independently released software components is nowadays widely supported by tools.
Among other benefits, the tools help guarantee backward compatibility of new versions of components. 
In other words, absence of non-breaking changes depends on ability of the tools to detect them.
But do the tools really cope well? This work aims at answering this question.  In brief, we have selected tools that analyse syntactic API changes
and benchmarked them. We provide results together with the synthetic data used for the test. The data set should be itself
a valuable contribution as it may test future tools.
\end{abstract}

% no keywords




% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle

\section{Introduction}

Quality assurance has been long understood as integral part of software development. While exercising programs either manually or by automated tests is still  common, more effective approaches come to light. The main rationale is to statically inspect the code and detect possible problems without costly invoking a program. Back in the beginning of millennia Jacobs pointed at challenges of static verification \cite{Jacobs2003formal} and claimed: 
\begin{quote}
"While abstract formalisms were useful at the time for explaining the main ideas in this field, they are not very helpful today when it comes to actual program verification for modern programming."
\end{quote}
Despite obstacles, both source and byte-code analysis started to be popular for Java program verifications. Over the last two decades a lot of approaches to byte-code verification have followed up. Let us name Leroy \cite{Leroy2003javabytecode} who reviews byte-code verification techniques which mainly concentrates on security and byte-code consistency issues. He was followed by others Male \cite{Male2008}, Klein \cite{Klein2003verified} or Burdy \cite{Burdy2006javabytecode} providing various byte-code based verifications. Noticeably, quite a few works \cite{Jezek2015detecting}, \cite{jezek2014ontheuse}, \cite{Raemaekers2011exploring}, \cite{Raemaekers2012measuring}, \cite{ebad2015measuring} target API evolution in standard Java, but also in Android \cite{Linares-Vasquez2013apichange}.

Approximately at the same time, static verification implemented in open source tools started to appear. Notoriously known are source-code style checkers such as \texttt{PMD}\footnote{https://github.com/pmd}, \texttt{Checkstyle}\footnote{http://checkstyle.sourceforge.net/}, or \texttt{Findbugs}\footnote{http://findbugs.sourceforge.net/} that are widely used and integrated into development tools or continuous integrations such us \texttt{Jenkins}\footnote{https://jenkins.io/} or \texttt{SonarQube}\footnote{http://www.sonarqube.org/}. 

Another set of tools are checkers of API evolution. They are generally useful for assessing compatibility of independently evolved libraries. They help developers assure smooth process of upgrading systems or releasing new versions of components. In contrast to the previous group, they work usually with byte-code and thus ideally fit for inspection of third-party binary libraries.  
All tools we are aware of are included in this work and listed in Table \ref{tab:tools}. They were collected putting together our knowledge, cross-refrences and search through developer forums, mostly \url{stackoverflow.com}. 

\todo[inline]{missing JDiff}
To our best knowledge, these tools are less popular and also seem to be  surrounded by smaller communities in comparison to the tools that check source-code style\footnote{One of the tool, \texttt{Clirr}, has three members, one contributor and finally is not developed since 2005. Still active is \texttt{Revapi} but with only four contributors. Bigger seems to be \texttt{japicmp} with 11 contributors. But they cannot match  \texttt{checkstyle} with 92 contributors or  \texttt{PMD} with 58}. It is interesting observation as API compatibility is not less important then code-style violations, or at least should not be. This work does not aim at answering if and why the tools are less popular, but it provides insight how they find API defects.

The variety of API changes is wide and the tools should ideally discover all of them. It is impractical to test the tools on real software as even a large system with a lot of versions does not have to contain all obscure API changes. Moreover, there are no meta-data about the changes that we could compare with tools results. For this reason, we created an artificial dataset in canonical form simulating syntactic changes of API. It is enriched with meta-data describing each change and its impact on compatibility. 

From the above mentioned, the contribution of this work is twofold. First, it provides an extensible dataset with possibly wide applications such as mock-testing, replication studies, or benchmark of new tools, etc. Secondly, we benchamrked how nowadays tools deal with detecting incompatibilities.  

\todo[inline]{finish}
Remainder of this paper is structured as follows ...


\section{Related Work}
In the technical domain, the term compatibility denotes\footnote{Source: the Merriam-Webster dictionary.} the ``ability to be used together'' and  ``designed to work with another device or system without modification''. Various definitions of compatibility related to software components exist, both in research \cite{canal2001compatibility,Belguidoum08formalization,taylor2009software,brada2011enhanced} % possibly stuckenholz06compatible, 
and technical \cite{forman1995release,osgi2010semvers,oracle2015compatibility} literature,
mostly dealing with the issue of either correct replacement or interoperability.

Belguidoum \cite{Belguidoum08formalization} distinguishes between \textit{vertical} and \textit{horizontal} compatibility which may be respectively paraphrased as backward and client-provider compatibility. Vertical compatibility plays role when vendors desire to produce backward compatible libraries, which allows for smooth system updates. On the other hand, horizontal compatibility plays role in checking system composition. 
Both these directions should be taken into account to successfully adopt ``units of  independent deployment and third-party composition'' defined in last decade by Szyperski \cite[4.1.1, p.36]{szyperski02component}. 


Compatibility may be expressed via sophisticated means such as non-functional properties \cite{Chung09OnNFRInSoftEn}, \cite{jezek13formalisation} and practically implemented in OSGi \cite{jezek2012tools-osgi}, Fractal \cite{bruneton2006fractal}, Sofa \cite{plasil2002behavior} or Treaty \cite{dietrich2009components}. It may be also expressed via contracts in the form of \textit{pre-} and \textit{post-conditions} introduced long time ago \cite{hoare1969axiomatic,Floyd67,Nau66} and implemented in languages such as famous Eiffel \cite{Meyer88}, research oriented Whiley~\cite{Pea13c,Pea15c} but also Java~\cite{DLNS98,FLLNSS02}.

There is also attempt to produce correct software right from the beginning, employing models that may be checked and guarantee a bug free product.
Let us name ProCom \cite{Sentilles_2009} which allows for modelling a system that is then generated into the code,
or SaveCCM \cite{Hanson2004SaveCCM} developed within the same cohort, enriched with visual means for easy system design.

Although these complex approach exist, API analysis to guarantee compatibility is still a popular means. Let us name Rama \cite{Rama2015structural} who claims:
``In this age of collaborative software development, the importance of usable APIs is well recognized'' and continues: ``ideally, the users of a module need to look no further than its API''. He proposes metrics that help either design or recognise a ``good'' API. 
API usability and design is moreover popularised by Myers and Stylos in \cite{Myers2016improving}.
Another work by Scheller \cite{Scheller2015automated} automatically measures usability of API in terms of interface complexity -- complexity of methods, constructors, fields, etc. He believes that this metric is the most important and still insufficiently handled. 
Sawant studied how API is used \cite{Sawant2015dataset}, developed a meta-model of API usage, provided a parser to collect data from open-source systems and made the data publicly available. We also share our results and scripts to stimulate follow-up research. 

To cope with API evolution, it is important to understand which changes are breaking. 
Overview of possible API breaking changes for Java is collected in  \cite{EvolvingJavaAPIs:2007}, which is a catalogue we also used to design our data set. 
Another work proposed by Cossette \cite{Cossette2012seeking} discusses available techniques to refactor clients for new API and checks how successfully these techniques actually adapt the client in practise. Another direction taken by Raemaekers \cite{Raemaekers2013testing} tries to correlate API breaking changes with several properties such us number of modifications. Taneja \cite{Taneja2007automated} tries to automatically find changed methods replacements by employing metrics such as name similarity, method size and closeness of method arguments. Nonetheless, research into API is huge, counting Eclipse platform, web and many more empirical studies \cite{Grechanik2010empirical,Dig2006howevolve,Kagdi2007survey,Businge2015eclipse,Espinha2014web,Espinha2015web}.

\section{Background: About Compatibility}

Consequence of a wrongly selected library is potential incompatibility with
its client. The notion of compatibility is complex
as every modification of a library may influence the way other libraries can use, interact, extend, observe or substitute it, in various ways.
However, practical applications usually rely on syntactic changes expressed via API. In other words, a syntactic change in API that does snot prevent client from linking or compiling is expected as compatible, even if it can have impact on behaviour. For instance, while a change from \texttt{List} to \texttt{Set} 
is acceptable for assignment to a field typed to \texttt{Collection}, the same change may impact clients that rely on particular elements order.

The Java Language Specification formally defines acceptable changes in respect to binary compatibility
\cite[ch. 13]{Java7Spec}:
\begin{quotation}
"a set of changes that developers are
permitted to make to a package or to a class or interface type while preserving (not
breaking) compatibility with pre-existing binaries."
\end{quotation} 
The rules are strictly defined with respect to the static analysis performed during linking, which
significantly differs from the notion of source compatibility, which is checked by the compiler as the consistency between a program and a library.
For this reason, the specification explicitly recommends: 
"tools for the Java programming language should support automatic recompilation."
In the same chapter it, however, admits that 
\begin{quotation}
"it is often impractical or impossible to automatically recompile
the pre-existing binaries that directly or indirectly depend on a type that is to be
changed."
\end{quotation}



% Kamil: removed as it is repreated in corpus discusion
%What is more, binary compatibility does not implies 
%source compatibility nor vice versa. For instance, specialising the return type 
%of a method is source compatible as the compiler converts types, but not binary 
%compatible as the linker expects exact types. On the other hand, changing generic type parameters is 
%often binary compatible due to erasures\footnote{But likely behaviour breaking, causing \texttt{ClassCastException} and similar as the erasures are replaced by the \texttt{checkcast} byte-code instruction.}, 
%but not source compatible as the compiler checks generic parameter bounds. 
%
%The discrepancy increases in time as new language features are differently projected into compiler and linker. Often, only the compiler is updated to handle new features, while the linker remains unchanged\footnote{Interesting counter-example are Lambdas that required modification of JVM while they could be implemented as syntactic constructs compiled to anonymous classes like byte-code compiled languages such as Scala.}.
%For instance, boxing and unboxing has been added in Java 1.5 to make working with primitive and wrapper types transparent for programmers, but the same conversion is not part of the linker. 

When a program is built and deployed, a mixed notion of compatibility is used. As the program is compiled, the source compatibility with the libraries 
is checked by the compiler. The binary compatibility is checked instead when the program is invoked. When not all directions of compatibility are satisfied, situations where a system may be compiled but cannot run or vice-versa may appear.

Compatibility also depends on how a library is used, 
i.e. if a library is used (invoked) only or also implemented (used for extension in sense of the object-oriented paradigm). For instance, a method added to an interface is acceptable for the client invoking this interface, but breaks compilation for someone implementing that interface as all methods must be implemented\footnote{Exceptions are Java 8 \texttt{default} methods}.
 
%Although API may be enriched with sophisticated behaviour-related annotations such as pre/post conditions \cite{liskov87data}, practical applications still rely on syntactic API expressing type system, i.e. method signatures. It allows for automation and easy verification by tools. The only question, to be answered in this work, is how current tools cope with finding API breaking changes. 

There were already works to catalogue API changes \cite{EvolvingJavaAPIs:2007} and a wide set of empirical studies \cite{jezek2014howjava}, \cite{dietrich14broken} or two works by Raemakers \cite{Raemaekers2011exploring}, \cite{Raemaekers2012measuring} to measure extend of the problem. Recently, we proposed even an approach to mitigate big part of the problem by employing run-time adaptation into Java \cite{jezek2016dynamo}. However, current tools have not bee obsoleted yet, as this research prototype is still waiting for its adoption by industry.


\section{API Changes Data Corpus}
\label{sec:corpus}

A few corpora with Java programs already exists. Well known is either Qualitas Corpus \cite{tempero2010qualitas} or DaCapo \cite{blackburn2006dacapo}. Both of them are impractical for usage in this work. Qualitas Corpus contain a lot of programs with evolving versions, but it has no meta-information about API changes and for this reason results of the tools could be hardly validated. DaCapo is much smaller and contain programs only in one version, preventing us from analysing API changes. We are not aware of another corpus fitting our needs and thus we created a new one.

Test data proposed in this work composes a corpus of possible syntactic API changes, which model evolution and releases of consequent versions of a library. 
The data are separated into eight top level categories: access modifiers, data types, exceptions, generics, inheritance, class members, other (non-access) modifiers and borderline (uncategorised) category. Each category is filled with examples of API changes, which were basically constructed following subsection is section 13 in the Java Language Specification, a catalogue created by Rivieres \cite{EvolvingJavaAPIs:2007} and a few more sources\footnote{Several developer forums such as stackoverflow.com}. The data does not have to be complete as some cases did not have to be know in the time, or new cases may appear as the language evolve. For this reason, the corpus is extendable and new examples may be added.

The corpus is split into three directories: \texttt{lib-v1}, \texttt{lib-v2} and \texttt{client}. As the names suggest, the directories contain a first (original) version a library, a second (evolved) version of a library and a client application, which invokes the library. The directories model real-life scenarios where a client uses libraries, but the libraries in the corpus are simplistic and contain only API with dummy implementations. The client application shows invocation of a library, thought it does not have to be exhaustive and more cases will actually exist.

Each library as well as the client hold a set of sub-directories, Java packages, representing concrete API evaluation. The package names are constructed following way:
\begin{verbatim}
<category><element><change>
\end{verbatim} 

Where \texttt{category} is one of the eight categories, \texttt{element} is a representation of a changed element (class, method, field, ...) and \texttt{change} describes content of the change. For instance, a case named \texttt{dataTypeClassFieldBoxing} means that a class field changed its data types and the concrete change was boxing.

The design based on the naming convention allows for extensibility of the corpus by simply adding new cases to sub-directories (packages) following the convention. In fact, the convention is not enforced, but recommended to keep order in the relatively big number of data -- the corpus currently contains 251 examples.

The corpus is provided in the form of source-code with an \texttt{ant} script to build binaries. It may be invoked simply by typing \texttt{ant} from the command-line.  The script output are three JAR files named the same way as the original source directories.

The whole structure of the corpus looks as follows (\texttt{<>} is shortcut for the \texttt{<category><element><change>} triplet described above):
\begin{verbatim}
<root>
  +- client/src/<>/Main.java
  +- lib-v1/src/lib/<>/<>.java
  +- lib-v2/src/lib/<>/<>.java
  build.xml
  compatibility.sh
\end{verbatim}


Although the corpus described so far may be used as such, we provide additional meta-data.
They must be generated before first usage to remain up-to date when the corpus is extended.
A linux bash script named \texttt{compatibility.sh} stored in the corpus root is provided to do this.
It generates a simple CSV file with three columns respectively listing: the name of a change as described above, and two columns informing about source and binary compatibility. Value ``$1$'' is printed for a compatible change, ``$0$'' otherwise. 

The script generates metadata by following steps, it:
\begin{enumerate}
\item reads all changes stored in the \texttt{client} directory
\item compiles \texttt{lib-v1} and \texttt{lib-v2} directories,
\item compiles the client against \texttt{lib-v1.jar} -- it should succeed, 
\item compiles the client against \texttt{lib-v2.jar} to check source compatibility-- it may fail,
\item invokes the client originally compiled against \texttt{lib-v1.jar} with \texttt{lib-v2.jar}  to check binary compatibility -- it may fail,
\item writes result (``1/0'') to the CSV file.
\end{enumerate}
Steps 4 and 5 fail when a respective change is source or binary incompatible, written in the last step into the CSV file.


We make the corpus publicly available as a GitHub project for replication studies, benchmark of new tools or similar: 
\begin{verbatim}
https://github.com/kjezek/
api-evolution-data-corpus/
\end{verbatim}

Following sub-sections detail the corpus separated into categories. Short discussions is provided in each sub-section to overview why the category is signification.

\subsection{Data Types}

Data types represent fundamental changes used for method and constructor signatures, field types, generics, inheritance and exceptions.
 
We cover changes respecting general features of object-oriented paradigm, i.e. polymorphisms as well as changes particular to Java. The basic changes are:
\begin{itemize}
\item Del -- a type is removed.
\item Inst -- a type is added. 
\item Gen -- generalised, i.e. type is moved up in an inheritance hierarchy. For instance, a \texttt{java.lang.Number} is generalisation of \texttt{java.lang.Integer}.
\item Spe -- specialised, which is opposite of the previous case.
\item Mut -- mutated, a type is changed to an incompatible one, i.e. not coming from the same inheritance tree.
\end{itemize}
Explicit cases to cover primitive types are modelled as well:
\begin{itemize}
\item Narrow -- a ``specialising'' conversion for primitive types. E.g. \texttt{long} changed to \texttt{int}.
\item Widen --  the opposite to the previous case.
\end{itemize}
Finally, Java allows for two more conversion to simplify work with primitive and wrapper types:
\begin{itemize}
\item Box -- a primitive type is converted to its wrapper type, e.g. \texttt{int} to \texttt{java.lang.Integer}.
\item Unbox -- a wrapper type converted to a primitive type.
\end{itemize}

Changes in this category play a different role for source and binary compatibility. In particular, \texttt{Gen}, \texttt{Spe}, \texttt{Narrow}, \texttt{Widen}, \texttt{Box} and \texttt{Unbox} are conversions performed only by the Java compiler, not the linker. It means, that they are always binary incompatible, but may be source compatible depending on usage. For instance, \texttt{Gen} is a source compatible conversion for a method parameter type, while \texttt{Spe} is compatible for a method return type. Example is a method that used to accept \texttt{java.lang.Integer} as a parameter type. It will remain compatible when it is changed to \texttt{java.lang.Number}. The opposite case holds for return types where the return type may be only specialised. However, the compiler decides correct conversions that s compiled into the byte-code and no more conversions are performed by the linker.

Changes \texttt{Del} and \texttt{Mut} are neither source nor binary compatible as no fall-back mechanism for non-existing or incompatible types is provided in Java.

\subsection{Exceptions}
Java distinguishes between so called checked and unchecked exceptions. The checked ones must be handled by a client code, either by propagation to upper levels or managed by the \texttt{try-catch} block. The unchecked exceptions are propagated automatically and may be optionally caught as well. 

It is less known that the handling of exceptions in the client code is checked only by the compiler, not the linker. 
As a consequence, changes in exceptions impact only source compatibility.
When a library method is updated so that it adds a new checked exception, the original client code cannot be compiled and it must be refactored to accommodate proper exception handling. On the other hand, if the same library is used in conjunction with an already compiled client, they will link smoothly together. 

The corpus combines examples of specialised, generalised or mutated exception types in variants for checked and unchecked ones. 

\subsection{Generics}

Generics were added to Java relatively late in version 1.5 with strong regard to compatibility with previous Java version. Their employment had required several simplifications, most noticeably so called \textit{erasures}. Generics are erased during the compilation from the caller site and their persist only on the target side. When the linker searches for types, it works as if only raw types without generics were used. 

Whereas a client code is checked by the compiler to correctly use generic types, a binary code that uses generics may be combined with the code not using generics thanks to erasures. The impact to compatibility is evident. A lot of changes that are binary compatible do not have to be source compatible. 

A typical example are bounds of a parametrized type: for instance, if a list is defined as \texttt{java.util.List<String>} only \texttt{String} values may be added and the compiler checks it. However, when the definition is changed to \texttt{java.util.List<Number>} and  only binaries updated, the system will successfully link. Let us note, that such a program will likely later rise \texttt{ClassCastException} or similar as erasures are effectively replaced by the \texttt{checkast} byte-code instruction.

\subsection{Inheritance}

Inheritance is sometimes not considered at all in respect to compatibility and some best practices discourage implementations/extensions of API classes \cite[p. 55]{Grand2002patterns}: ``if a class is declared as a subclass, there is risk that these classes not under your control will change in an incompatible way''. In other words, the API must be almost frozen to be backward compatible also for inheritance. 

Some changes such as removed methods clearly break compatibility, but some breaking changes are less evident. Addition of an method to an interface is still a quite known example, but a possible compatibility issue of making a method more visible is, however, less obvious. In detail, changing visibility e.g. from \texttt{private} to \texttt{public} may seem harmless as only more is provided, but the new public method may overlap with the same method in a sub-class. When the sub-class method has a stricter access, the compilation also fails as the access cannot be weakened through inheritance. As a result, adding a public method to a class may break compatibility for inheritance.

Since the changes possibly impacting inheritance are partly covered by other categories (method, modifier, types etc. changes), this category contributes to the corpus with several more examples with class/interface definitions modified in a sub-class. Several examples with methods moved up and down in the hierarchy tree are provided as well.

\subsection{Members}

Members are elements defined in a Java class including fields, methods and constructors. This category contains examples of removed or added members that have expected impact on compatibility: removed elements are always incompatible, added elements may be incompatible for inheritance as discussed above. 

The problem of added abstract/interface methods addressed in Java 1.8 by \texttt{default} methods is modelled in the data set as well.

\subsection{Access Modifiers}

Access modifiers may be either weakened or strengthened and may be applied to a constructor, a method, a field, a class and an interface. These combinations are reflected in the corpus. 

A change making an element more accessible is usually a compatible change while restricting the access is incompatible. It does not differ for source and binary compatibility. The only known exception is inheritance of a method with a weaker access as already discussed. 

\subsection{Other Modifiers}

Other, non-access, modifiers have various purposes in Java and for this reason have different impact to compatibility. Modifier \texttt{volatile}, \texttt{transient}, \texttt{native} or \texttt{strictfp} are denoted to specific treatment of respective elements, \texttt{final} or \texttt{abstract} are used in conjunction with inheritance, and \texttt{static} deals with access context. Sometimes one modifier is used for multiple purposes, e.g. constants are implemented as \texttt{final} fields, while \texttt{final} also denotes classes that cannot be inherited. 

There is no pattern how these modifiers impact compatibility. For instance, a tagging modifier \texttt{transient} does not break compatibility while \texttt{native} does. It is because \texttt{native} requires a special treatment by JVM while \texttt{transient} is only a meta-information. Modifiers \texttt{final} and \texttt{abstract} have obvious effect as their adding or removing may break inheriting classes. 

Interesting is the \texttt{static} modifier that may be in certain cases source compatible and binary incompatible. It is more discussed in \cite[Section 4]{jezek2014howjava}: static elements, fields and methods, may be invoked from non-static context (via a reference) and pass the compilation. However, this combination is forbidden at runtime as different byte-code instructions are generated for static and non-static access and thus it fails when the byte-code is updated without recompilation.

\subsection{Borderline Cases}

Java contains several specific features that are grouped here. Notoriously know is implicit inheritance of \texttt{Object}  by any classes.  Maybe less known is that any Java array by default implements \texttt{Cloneable} and \texttt{Serialisable}. Consequence is that possible specialisation between user classes and these classes must be taken into account when deciding compatibility.

Another example in this category is a change from class to interface or vice-versa also detailed in \cite[Section 4]{jezek2014howjava}. It is interesting because invocation of methods does not differ between class and interface methods in the source-code. However, different byte-code instructions are generated leading to another discrepancy between source and binary compatibility. In other words, when a library class is changed to an interface or vice-versa, the client invoking methods from this interface/class may be recompiled, but cannot be linked without recompilation.


\section{Tools Challenge}

We searched for tools that are capable of discovering API syntactic backward compatibility and included them into benchmarkk. They are listed in Table \ref{tab:tools} together with basic information about their authors, current versions, licensing and basic usages. All these tools were benchmarked to find out how they cope with finding incompatibilities, results are provided below.

\begin{table*}[t]
  \centering
  \begin{tabular}{l | p{1cm} p{1cm} p{1cm} p{1cm} p{1cm} p{1cm} p{1cm} p{1cm} p{1cm} }
Tool  &  Clirr  &  Japicmp  &  japiChecker  &  JAPICC  &  Revapi  &  Sigtest  &  Japitools  &  Jour  &  JaCC	\\
\hline
\multicolumn{10}{|c|}{Basic info} \\
\hline
Author  &  Lars Kühne  &  Martin Mois  &  William Bernardet  &  Andrey Ponomarenko  &  Lukas Krejci  &  Oracle  &  Stuart Ballard  &  Vlad Skarzhevskyy  &  UWB	\\
License  &  LGPL  &  A2.0  &  A2.0  &  LGPL  &  A2.0  &  GPLv2   &  GPL   &  LGPL  &  ask	\\
Version  &  0.6.0  &  0.7.2  &  0.2.1  &  1.5  &  0.4.2  &  3.1  &  0.9.7  &  2.0.3  &  1.0.9	\\
Release  &  9/27/2005  &  3/20/2016  &  10/3/2015  &  4/8/2016  &  3/30/2016  &  4/8/2016  &  11/13/2007  &  12/12/2008  &  	\\
\hline
\multicolumn{10}{|c|}{Output} \\
\hline
TXT  &  yes  &  yes  &  yes  &    &  yes  &  yes  &  yes  &  yes  &  yes	\\
XML  &  yes  &  yes  &    &    &    &    &    &    &  	\\
HTML  &  yes  &  yes  &    &  yes  &    &    &    &    &  	\\
\hline
\multicolumn{10}{|c|}{Integration} \\
\hline
CLI  &  yes  &  yes  &  yes  &  yes  &  yes  &  yes  &  yes  &  yes  &  	\\
Maven  &  yes  &  yes  &  yes  &    &  yes  &  yes  &    &  yes  &  yes	\\
Ant  &  yes  &    &  yes  &    &  yes  &  yes  &    &    &  		\\
libray  &    &  yes  &    &    &    &    &    &    &  yes	\\
\end{tabular}

\caption{Tested Tools (GPL//LGPL = GNU GPL/LGPL, A2.0 = Apache 2.0)}
  \label{tab:tools}
\end{table*}

\subsection{Methodology}

The tools were tested using following approach: 
we generated the meta-data CSV file as described in Section \ref{sec:corpus}, then we removed lines representing only compatibilities. After that, we invoked all the tools and redirected their output to text files. In certain cases we removed lines from outputs representing a compatible change. Finally, we iterated the meta-data CSV file and used string matching to find changes in tools outputs. We collected the results in another CSV file, which lists changes in lines and tools in columns. The columns contain ``$1$'' for a correctly detected incompatibility and ``$0$'' meaning that a tool did not find the incompatibility.

The lines with compatible changes were removed from the original CSV meta-data to prevent false negatives.
We had to analyse only provably incompatible changes because the current client does not have to handle all possible invocations. An incompatible usage not modelled in the client may exist and if detected by a tool, it would not match with the expected compatibility leading to a wrong result. 

Compatible changes were removed from the tools outputs to prevent false positives. 
Some of the tools list all API they crawled with additional compatibility classification. It could cause an incorrect string matching if a change not recognised as incompatible were listed in the output. This step differs for each tool. Some of the tools output only incompatibilities and do not have to be corrected (\texttt{japichecker}, \texttt{japicc}) while some have to. Usually such lines can be easily caught by a simple regular expression as they contain representative strings such as char \texttt{!} (\texttt{japicmp}), text \texttt{100\% Compatible} (\texttt{japitool}), \texttt{NON\_BREAKING} (\texttt{revapi}), \texttt{INFO} (\texttt{clirr}).  

\subsection{Extendability}
The whole process is automated and may be invoked by a bash script \texttt{./benchmark.sh}. The script prepares the meta-data, invokes the tools, corrects outputs and analyse results. It delegates invocation of the tools to the script \texttt{tools/run.sh}, which executes all tools one-by-one.

For instance, the \texttt{run.sh} script contains following lines to invoke \texttt{japicmp}:
\begin{verbatim}
REPORTS=".reports"
java -jar japicmp/japicmp-0.7.2.jar \
  -o ../lib-v1.jar \
  -n ../lib-v2.jar \
  -a private > "$REPORTS"/japicmp.txt
  
grep -v '===  UNCHANGED' \
  "$REPORTS"/japicmp.txt > japicmp.txt.tmp 
mv japicmp.txt.tmp "$REPORTS"/japicmp.txt   
\end{verbatim}

New tools may be added to the benchmark simply by adding invocable lines to this script so that they produce a textual output stored in the \texttt{tools/.reports}.

The structure of the corpus including the tools benchmark looks like:
\begin{verbatim}
<root>
  +- client/src/<>/Main.java
  +- lib-v1/src/lib/<>/<>.java
  +- lib-v2/src/lib/<>/<>.java
  +- tools/.reports
  +- tools/<tool>
  +- tools/run.sh
  build.xml
  compatibility.sh
  benchmark.sh
\end{verbatim}

\subsection{Results}

The result of described experiment has shown that the tools widely differ in their ability to find compatibility breaking changes. Results are provided in Table \ref{tab:result-categories}
as percentages of successfully detected compatibility breaking changes. They are separated to categories with a summary in the last row. 

While the results show clearly that the worst tool is \texttt{clirr} and the best is \texttt{sigtest}, detailed analysis reveals that \texttt{clirr} may be a better choice than some of the better performing tools in certain use-cases.

\texttt{Clirr} is not actively developed since 2005, and evidently does not recognize generics and exceptions. However, it works well in other categories and may be still useful for detecting only binary incompatible changes as generics and exceptions impacts only srouce compatibility.
Similar situation appears for \texttt{japicmp} with overall a poor result, which is however caused by unsporting generics and a few bugs in detecting modifiers. Otherwise, the tool works well.

The second place is occupied by \texttt{japitool} which also seems to be no more developed since 2006, thought still available as part of Linux distributions (Debian/Linux in particular).

Tools  such as \texttt{japicc} or \texttt{revapi} showed overall better score, but they have several issues scattered among categories. They may be less reliable in production as they can miss important issues in both categories of source and binary compatibility, while worse performing tools like \texttt{clirr} or \texttt{japicmp} may be a better choice when source compatibility is out of interest. Nonetheless, both tools are still developed and may be thus improved in the future. 


\texttt{Sigtest} wins the benchmark as it is able to detect almost all problems. It fails only in two changes, detection of the removed \texttt{strictfp} modifier and addition of the \texttt{native} modifier, which are both binary incompatible. As they are very specific modifiers, we do not expect their frequent changes among library versions. For this reason, \texttt{sigtest} may serve as the most reliable tool from this benchmark.
  
Table \ref{tab:result-types} provides insight into results separated for source and binary incompatibilities. First line shows changes that are only source incompatible and binary compatible. The second line in contrast lists changes that are binary incompatible, but may be either source compatible or incompatible. 
We separated the data this way to test the tools specifically for source compatibility and for the rest.

The table provides interesting results, the tools perform much better in detecting binary incompatibilities.  Worst in this category is \texttt{revapi} while most of the tools detected more than $90\%$ of issues. On the other hand, the tools lack ability to detect source incompatibility. The only actually reliable tools are \texttt{sigtest} and \texttt{japitool} that correctly recognised all source incompatibilities. Partly useful is \texttt{revapi} with about 88\% of successful results.  Other tools detected only a small number of problems and cannot be recommended.

Non-functional properties such as easiness of usage is nonetheless important in recommending the tools. A tool with a few bugs may be a better choice if it provides a better user comfort. While we did not measured it systematically in this work, we did some observation about tools usage and output. It must be said that the tools are very similar in this aspect. All provide CLI with a few options to input JAR files and print out a human readable formatted TXT output. We see non of the formats particularly better then another one. One exception is \texttt{japicc} which provides a HTML output classifying severity of changes, which may be a better readable by humans and especially by non-programmers. 

To summarize, the experiment shows that the most usable tool is \texttt{sigtest}, which is distributed as open-source and may be easily integrated into development process via CLI, Maven or Ant plugin. Furthermore, it was detected that other tools are in reality usable only for checking binary compatibility. Nonetheless, it may be sufficient in many scenarios as library updates are usually distributed in binary forms.  Hence, binary compatibility checking may help find the most unpredictable runtime failures caused by opaque third-party libraries. 
Although a source incompatible change may break a system as well, it is detectable by project build early in the development phase and thus it is less harmful.


\begin{table*}[t]
  \centering
  \begin{tabular}{l | r r r r r r r r r }
Category   &  clirr   &  jacc   &  japicc   &  japiChecker   &  japicmp   &  japitool   &  jour   &  revapi   &  sigtest  \\
\hline
Access Modifiers   &  100.00\%   &  100.00\%   &  83.33\%   &  100.00\%   &  100.00\%   &  100.00\%   &  83.33\%   &  83.33\%   &  100.00\% \\
Data Types   &  100.00\%   &  100.00\%   &  89.36\%   &  100.00\%   &  100.00\%   &  100.00\%   &  100.00\%   &  95.74\%   &  100.00\%  \\
Exceptions   &  0.00\%   &  0.00\%   &  100.00\%   &  100.00\%   &  100.00\%   &  100.00\%   &  100.00\%   &  71.43\%   &  100.00\%  \\
Generics   &  0.00\%   &  33.33\%   &  5.88\%   &  0.00\%   &  0.00\%   &  100.00\%   &  17.65\%   &  100.00\%   &  100.00\%  \\
Inheritance   &  71.43\%   &  100.00\%   &  71.43\%   &  85.71\%   &  100.00\%   &  100.00\%   &  100.00\%   &  42.86\%   &  100.00\%  \\
Members   &  100.00\%   &  100.00\%   &  84.21\%   &  89.47\%   &  100.00\%   &  100.00\%   &  84.21\%   &  42.11\%   &  100.00\%  \\
Other Modifiers   &  61.54\%   &  84.62\%   &  84.62\%   &  53.85\%   &  84.62\%   &  69.23\%   &  76.92\%   &  61.54\%   &  84.62\%  \\
Others   &  100.00\%   &  100.00\%   &  75.00\%   &  100.00\%   &  100.00\%   &  100.00\%   &  100.00\%   &  50.00\%   &  100.00\%  \\
\hline
Total  &  57.79\%   &  72.08\%   &  59.74\%   &  61.04\%   &  65.58\%   &  97.40\%   &  68.18\%   &  82.47\%   &  98.70\%  \\  
\end{tabular}
  \caption{Correctly Detected Incompatibilities}
  \label{tab:result-categories}
\end{table*}

\begin{table*}[t]
  \centering
  \begin{tabular}{l | r r r r r r r r r }
Type   &  clirr   &  jacc   &  japicc   &  japiChecker   &  japicmp   &  japitool   &  jour   &  revapi   &  sigtest  \\
\hline
Source 	& 13.24\%	&	41.18\%	&	25.00\%	&	20.59\%		&	25.00\%		&	100.00\%	&	38.24\%	&	88.24\%	&	100.00\%  \\
Binary	& 93.02\%	&	96.51\%	&	87.21\%	&	93.02\%		&	97.67\%		&	95.35\%		&	91.86\%	&	77.91\%	&	97.67\%		\\
%Both  	&  57.79\%   &  72.08\%   &  59.74\%   &  61.04\%   &  65.58\%   &  97.40\%   &  68.18\%   &  82.47\%   &  98.70\%  \\  
\end{tabular}
  \caption{Source vs Binary Incompatibilities}
  \label{tab:result-types}
\end{table*}


\subsection{Threats to Validity}

\todo[inline]{do we need threats? What is in reality threat here?}
- data completeness
- bugs in data analysis

\section{Conclusion}

This work has investigated how available open-source tools cope with detecting API incompatible changes. We found that the tools vary, but it is not a problem to find a reliable one. The best available is \texttt{Sigtest} which successfully detected all source incompatible changes and missed only two binary incompatibilities. Other tools vary in how successfully their find binary and source compatibility and for this reason they may be recommended only depending on their usage. 

We have also created and made publicly available a data corpus, which may be used for other studies. Discussion accompanying the data moreover provides a valuable insight into compatibility obstacles in Java. 

In the future, we would like to extend the data of more examples with inheritance, which is only partly covered. We would also like to concentrate on tools that checks the client-library relation. Although we see this direction nonetheless important, our preliminary research shows that the amount of existing tools is very smaller. 


% conference papers do not normally have an appendix


% use section* for acknowledgment
\section*{Acknowledgment}


The authors would like to thank...


\bibliographystyle{plain}
\bibliography{references}

\end{document}


